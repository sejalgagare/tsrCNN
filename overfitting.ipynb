{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-6\n",
    "### Overfit designed CNNs models in Keras\n",
    "\n",
    "**Description:**  \n",
    "*Implement overfitting with small amount of images from prepared datasets  \n",
    "Plot resulted charts*\n",
    "\n",
    "**File:** *overfitting.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Open preprocessed dataset  \n",
    "**--> Step 2:** Convert classes vectors to binary matrices  \n",
    "**--> Step 3:** Load saved CNN model  \n",
    "**--> Step 4:** Set up learning rate & epochs  \n",
    "**--> Step 5: Overfit loaded CNN model**  \n",
    "**--> Step 6:** Show and plot accuracies  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Plot with training and validation accuracies  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    'D:\\Programming\\Jupiter Notebook\\CNNCourse\\Section4'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section5' with designed models\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section5'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section5'\n",
    "full_path_to_Section5 = \\\n",
    "    'D:\\Programming\\Jupiter Notebook\\CNNCourse\\Section5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x_test', 'x_train', 'x_validation', 'y_test', 'y_train', 'y_validation']\n"
     ]
    }
   ],
   "source": [
    "# Opening saved custom dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' +\n",
    "               'dataset_custom_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables\n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "(3240, 64, 64, 3)\n",
      "(3240,)\n",
      "(1110, 64, 64, 3)\n",
      "(1110,)\n",
      "(278, 64, 64, 3)\n",
      "(278,)\n"
     ]
    }
   ],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class index from vector: 2\n",
      "\n",
      "(3240, 5)\n",
      "(1110, 5)\n",
      "\n",
      "Class index from matrix: [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 5)\n",
    "y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is successfully loaded\n"
     ]
    }
   ],
   "source": [
    "# Loading 1st model for RGB datasets\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model = load_model(full_path_to_Section5 + '/' + 'custom' + '/' + 'model_1_custom_rgb.h5')\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 64)        4864      \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 32, 32, 64)       0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 128)       204928    \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 16, 16, 128)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "                                                                 \n",
      " average_pooling2d_2 (Averag  (None, 8, 8, 256)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 512)         3277312   \n",
      "                                                                 \n",
      " average_pooling2d_3 (Averag  (None, 4, 4, 512)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              16779264  \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 10245     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,096,069\n",
      "Trainable params: 21,096,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Dropout rate:  0.2\n",
      "Strides of the 1st convolutional layer:  (1, 1)\n",
      "Strides of the average pooling layer:  (2, 2)\n",
      "\n",
      "Full configuration details of the 1st layer:\n",
      " {'class_name': 'InputLayer', 'config': {'batch_input_shape': (None, 64, 64, 3), 'dtype': 'float32', 'sparse': False, 'ragged': False, 'name': 'conv2d_input'}}\n"
     ]
    }
   ],
   "source": [
    "# Showing model's summary in table format\n",
    "print(model.summary())\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing dropout rate\n",
    "print('Dropout rate: ', model.layers[2].rate)\n",
    "\n",
    "# Showing strides for the 1st layer (convolutional)\n",
    "print('Strides of the 1st convolutional layer: ', model.layers[0].strides)\n",
    "\n",
    "# Showing strides for the 2nd layer (average pooling)\n",
    "print('Strides of the average pooling layer: ', model.layers[1].strides)\n",
    "print()\n",
    "\n",
    "# Showing configurations for entire model\n",
    "# print(model.get_config())\n",
    "\n",
    "# Showing configurations for specific layers\n",
    "print('Full configuration details of the 1st layer:\\n', model.get_config()['layers'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epochs and schedule for learning rate are set successfully\n"
     ]
    }
   ],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Overfitting loaded CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 7.694497527671315e-05.\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.5537 - accuracy: 0.2600 - val_loss: 1.4857 - val_accuracy: 0.2480 - lr: 7.6945e-05\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 7.30977265128775e-05.\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 21s 211ms/step - loss: 1.4557 - accuracy: 0.3200 - val_loss: 1.3186 - val_accuracy: 0.4300 - lr: 7.3098e-05\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 6.94428401872336e-05.\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 21s 215ms/step - loss: 1.3604 - accuracy: 0.4100 - val_loss: 1.2715 - val_accuracy: 0.4600 - lr: 6.9443e-05\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 6.597069817787194e-05.\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 1.2332 - accuracy: 0.5200 - val_loss: 1.3112 - val_accuracy: 0.4700 - lr: 6.5971e-05\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 6.267216326897833e-05.\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 1.0714 - accuracy: 0.5900 - val_loss: 1.3289 - val_accuracy: 0.4480 - lr: 6.2672e-05\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 5.953855510552941e-05.\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 21s 216ms/step - loss: 0.9892 - accuracy: 0.6200 - val_loss: 1.3626 - val_accuracy: 0.4740 - lr: 5.9539e-05\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 5.656162735025293e-05.\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 21s 212ms/step - loss: 0.8067 - accuracy: 0.6700 - val_loss: 1.6790 - val_accuracy: 0.4440 - lr: 5.6562e-05\n",
      "\n",
      "Epoch 00008: LearningRateScheduler setting learning rate to 5.373354598274029e-05.\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 0.7368 - accuracy: 0.6900 - val_loss: 1.6773 - val_accuracy: 0.4540 - lr: 5.3734e-05\n",
      "\n",
      "Epoch 00009: LearningRateScheduler setting learning rate to 5.1046868683603266e-05.\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 21s 215ms/step - loss: 0.5754 - accuracy: 0.7400 - val_loss: 1.8011 - val_accuracy: 0.4580 - lr: 5.1047e-05\n",
      "\n",
      "Epoch 00010: LearningRateScheduler setting learning rate to 4.8494525249423105e-05.\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 0.4778 - accuracy: 0.8500 - val_loss: 1.9103 - val_accuracy: 0.4220 - lr: 4.8495e-05\n",
      "\n",
      "Epoch 00011: LearningRateScheduler setting learning rate to 4.6069798986951947e-05.\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 0.2714 - accuracy: 0.9200 - val_loss: 2.1852 - val_accuracy: 0.4120 - lr: 4.6070e-05\n",
      "\n",
      "Epoch 00012: LearningRateScheduler setting learning rate to 4.3766309037604346e-05.\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 0.1902 - accuracy: 0.9500 - val_loss: 2.6822 - val_accuracy: 0.4080 - lr: 4.3766e-05\n",
      "\n",
      "Epoch 00013: LearningRateScheduler setting learning rate to 4.157799358572413e-05.\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 0.0897 - accuracy: 0.9600 - val_loss: 3.1718 - val_accuracy: 0.4360 - lr: 4.1578e-05\n",
      "\n",
      "Epoch 00014: LearningRateScheduler setting learning rate to 3.949909390643792e-05.\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 0.0711 - accuracy: 0.9700 - val_loss: 3.5865 - val_accuracy: 0.4260 - lr: 3.9499e-05\n",
      "\n",
      "Epoch 00015: LearningRateScheduler setting learning rate to 3.752413921111602e-05.\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 3.8492 - val_accuracy: 0.4220 - lr: 3.7524e-05\n",
      "\n",
      "Epoch 00016: LearningRateScheduler setting learning rate to 3.564793225056022e-05.\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 21s 215ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 4.2035 - val_accuracy: 0.4280 - lr: 3.5648e-05\n",
      "\n",
      "Epoch 00017: LearningRateScheduler setting learning rate to 3.386553563803221e-05.\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 4.4486 - val_accuracy: 0.4300 - lr: 3.3866e-05\n",
      "\n",
      "Epoch 00018: LearningRateScheduler setting learning rate to 3.217225885613059e-05.\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 4.6359 - val_accuracy: 0.4260 - lr: 3.2172e-05\n",
      "\n",
      "Epoch 00019: LearningRateScheduler setting learning rate to 3.0563645913324066e-05.\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 4.8352 - val_accuracy: 0.4280 - lr: 3.0564e-05\n",
      "\n",
      "Epoch 00020: LearningRateScheduler setting learning rate to 2.903546361765786e-05.\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 4.9085 - val_accuracy: 0.4300 - lr: 2.9035e-05\n",
      "\n",
      "Epoch 00021: LearningRateScheduler setting learning rate to 2.7583690436774966e-05.\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 5.0291 - val_accuracy: 0.4260 - lr: 2.7584e-05\n",
      "\n",
      "Epoch 00022: LearningRateScheduler setting learning rate to 2.6204505914936218e-05.\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 5.1616 - val_accuracy: 0.4280 - lr: 2.6205e-05\n",
      "\n",
      "Epoch 00023: LearningRateScheduler setting learning rate to 2.4894280619189404e-05.\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 5.2580 - val_accuracy: 0.4340 - lr: 2.4894e-05\n",
      "\n",
      "Epoch 00024: LearningRateScheduler setting learning rate to 2.3649566588229933e-05.\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 8.3143e-04 - accuracy: 1.0000 - val_loss: 5.3719 - val_accuracy: 0.4300 - lr: 2.3650e-05\n",
      "\n",
      "Epoch 00025: LearningRateScheduler setting learning rate to 2.2467088258818436e-05.\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 5.4311 - val_accuracy: 0.4260 - lr: 2.2467e-05\n",
      "\n",
      "Epoch 00026: LearningRateScheduler setting learning rate to 2.134373384587751e-05.\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 7.6796e-04 - accuracy: 1.0000 - val_loss: 5.5315 - val_accuracy: 0.4300 - lr: 2.1344e-05\n",
      "\n",
      "Epoch 00027: LearningRateScheduler setting learning rate to 2.0276547153583635e-05.\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 6.9210e-04 - accuracy: 1.0000 - val_loss: 5.6123 - val_accuracy: 0.4320 - lr: 2.0277e-05\n",
      "\n",
      "Epoch 00028: LearningRateScheduler setting learning rate to 1.9262719795904453e-05.\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 8.1886e-04 - accuracy: 1.0000 - val_loss: 5.6425 - val_accuracy: 0.4320 - lr: 1.9263e-05\n",
      "\n",
      "Epoch 00029: LearningRateScheduler setting learning rate to 1.8299583806109228e-05.\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 5.5465e-04 - accuracy: 1.0000 - val_loss: 5.7169 - val_accuracy: 0.4260 - lr: 1.8300e-05\n",
      "\n",
      "Epoch 00030: LearningRateScheduler setting learning rate to 1.7384604615803768e-05.\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 3.2755e-04 - accuracy: 1.0000 - val_loss: 5.7655 - val_accuracy: 0.4300 - lr: 1.7385e-05\n",
      "\n",
      "Epoch 00031: LearningRateScheduler setting learning rate to 1.651537438501358e-05.\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 5.3356e-04 - accuracy: 1.0000 - val_loss: 5.7821 - val_accuracy: 0.4300 - lr: 1.6515e-05\n",
      "\n",
      "Epoch 00032: LearningRateScheduler setting learning rate to 1.56896056657629e-05.\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 3.9839e-04 - accuracy: 1.0000 - val_loss: 5.8337 - val_accuracy: 0.4300 - lr: 1.5690e-05\n",
      "\n",
      "Epoch 00033: LearningRateScheduler setting learning rate to 1.4905125382474753e-05.\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 2.9194e-04 - accuracy: 1.0000 - val_loss: 5.8753 - val_accuracy: 0.4280 - lr: 1.4905e-05\n",
      "\n",
      "Epoch 00034: LearningRateScheduler setting learning rate to 1.4159869113351015e-05.\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 21s 209ms/step - loss: 3.2599e-04 - accuracy: 1.0000 - val_loss: 5.8996 - val_accuracy: 0.4320 - lr: 1.4160e-05\n",
      "\n",
      "Epoch 00035: LearningRateScheduler setting learning rate to 1.3451875657683464e-05.\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 21s 210ms/step - loss: 2.2644e-04 - accuracy: 1.0000 - val_loss: 5.9316 - val_accuracy: 0.4320 - lr: 1.3452e-05\n",
      "\n",
      "Epoch 00036: LearningRateScheduler setting learning rate to 1.2779281874799288e-05.\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 21s 209ms/step - loss: 5.3424e-04 - accuracy: 1.0000 - val_loss: 5.9226 - val_accuracy: 0.4320 - lr: 1.2779e-05\n",
      "\n",
      "Epoch 00037: LearningRateScheduler setting learning rate to 1.2140317781059324e-05.\n",
      "Epoch 37/50\n",
      " 45/100 [============>.................] - ETA: 6s - loss: 1.9985e-04 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model.fit(x_train[:100], y_train[:100],\n",
    "                        batch_size=1,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation[:500], y_validation[:500]),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB custom dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the model\n",
    "print('Training accuracy={0:.5f}, Validation accuracy={1:.5f}'.\n",
    "                                                             format(max(h.history['accuracy']),\n",
    "                                                                    max(h.history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h.history['accuracy'], '-o')\n",
    "plt.plot(h.history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['training accuracy', 'validation accuracy'], loc='center right', fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Overfitting model for Custom Dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('overfitted_model_of_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' +\n",
    "               'dataset_cifar10_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading 1st model for RGB datasets\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model = load_model(full_path_to_Section5 + '/' + 'cifar10' + '/' + 'model_1_cifar10_rgb.h5')\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Showing model's summary in table format\n",
    "print(model.summary())\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing dropout rate\n",
    "print('Dropout rate: ', model.layers[2].rate)\n",
    "\n",
    "# Showing strides for the 1st layer (convolutional)\n",
    "print('Strides of the 1st convolutional layer: ', model.layers[0].strides)\n",
    "\n",
    "# Showing strides for the 2nd layer (max pooling)\n",
    "print('Strides of the max pooling layer: ', model.layers[1].strides)\n",
    "print()\n",
    "\n",
    "# Showing configurations for entire model\n",
    "# print(model.get_config())\n",
    "\n",
    "# Showing configurations for specific layers\n",
    "print('Full configuration details of the 1st layer:\\n', model.get_config()['layers'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Overfitting loaded CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model.fit(x_train[:100], y_train[:100],\n",
    "                        batch_size=1,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation[:500], y_validation[:500]),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB CIFAR-10 dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the model\n",
    "print('Training accuracy={0:.5f}, Validation accuracy={1:.5f}'.\n",
    "                                                             format(max(h.history['accuracy']),\n",
    "                                                                    max(h.history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h.history['accuracy'], '-o')\n",
    "plt.plot(h.history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['training accuracy', 'validation accuracy'], loc='upper left', fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Overfitting model for CIFAR-10 Dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('overfitted_model_of_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved MNIST dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' +\n",
    "               'dataset_mnist_gray_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading 1st model for GRAY datasets\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model = load_model(full_path_to_Section5 + '/' + 'mnist' + '/' + 'model_1_mnist_gray.h5')\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Showing model's summary in table format\n",
    "print(model.summary())\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing dropout rate\n",
    "print('Dropout rate: ', model.layers[2].rate)\n",
    "\n",
    "# Showing strides for the 1st layer (convolutional)\n",
    "print('Strides of the 1st convolutional layer: ', model.layers[0].strides)\n",
    "\n",
    "# Showing strides for the 2nd layer (convolutional with strides 2)\n",
    "print('Strides of the 2nd convolutional layer: ', model.layers[1].strides)\n",
    "print()\n",
    "\n",
    "# Showing configurations for entire model\n",
    "# print(model.get_config())\n",
    "\n",
    "# Showing configurations for specific layers\n",
    "print('Full configuration details of the 1st layer:\\n', model.get_config()['layers'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Overfitting loaded CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model.fit(x_train[:20], y_train[:20],\n",
    "                        batch_size=1,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation[:500], y_validation[:500]),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRAY MNIST dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the model\n",
    "print('Training accuracy={0:.5f}, Validation accuracy={1:.5f}'.\n",
    "                                                             format(max(h.history['accuracy']),\n",
    "                                                                    max(h.history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h.history['accuracy'], '-o')\n",
    "plt.plot(h.history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['training accuracy', 'validation accuracy'], loc='center right', fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Overfitting model for MNIST Dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('overfitted_model_of_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 1: Opening preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "# Initiating File object\n",
    "# Opening file in reading mode by 'r'\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' +\n",
    "               'dataset_ts_rgb_255_mean_std.hdf5', 'r') as f:\n",
    "    \n",
    "    # Showing all keys in the HDF5 binary file\n",
    "    print(list(f.keys()))\n",
    "    \n",
    "    # Extracting saved arrays for training by appropriate keys\n",
    "    # Saving them into new variables    \n",
    "    x_train = f['x_train']  # HDF5 dataset\n",
    "    y_train = f['y_train']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_train = np.array(x_train)  # Numpy arrays\n",
    "    y_train = np.array(y_train)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for validation by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_validation = f['x_validation']  # HDF5 dataset\n",
    "    y_validation = f['y_validation']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_validation = np.array(x_validation)  # Numpy arrays\n",
    "    y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Extracting saved arrays for testing by appropriate keys\n",
    "    # Saving them into new variables \n",
    "    x_test = f['x_test']  # HDF5 dataset\n",
    "    y_test = f['y_test']  # HDF5 dataset\n",
    "    # Converting them into Numpy arrays\n",
    "    x_test = np.array(x_test)  # Numpy arrays\n",
    "    y_test = np.array(y_test)  # Numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing types of loaded arrays\n",
    "print(type(x_train))\n",
    "print(type(y_train))\n",
    "print(type(x_validation))\n",
    "print(type(y_validation))\n",
    "print(type(x_test))\n",
    "print(type(y_test))\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing shapes of loaded arrays\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 2: Converting classes vectors to classes matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing class index from the vector\n",
    "print('Class index from vector:', y_train[5])\n",
    "print()\n",
    "\n",
    "# Preparing classes to be passed into the model\n",
    "# Transforming them from vectors to binary matrices\n",
    "# It is needed to set relationship between classes to be understood by the algorithm\n",
    "# Such format is commonly used in training and predicting\n",
    "y_train = to_categorical(y_train, num_classes = 43)\n",
    "y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "\n",
    "\n",
    "# Showing shapes of converted vectors into matrices\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing class index from the matrix\n",
    "print('Class index from matrix:', y_train[5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 3: Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading 1st model for RGB datasets\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "model = load_model(full_path_to_Section5 + '/' + 'ts' + '/' + 'model_1_ts_rgb.h5')\n",
    "\n",
    "# Check point\n",
    "print('Model is successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's summary in table format\n",
    "print(model.summary())\n",
    "print()\n",
    "\n",
    "\n",
    "# Showing dropout rate\n",
    "print('Dropout rate: ', model.layers[2].rate)\n",
    "\n",
    "# Showing strides for the 1st layer (convolutional)\n",
    "print('Strides of the 1st convolutional layer: ', model.layers[0].strides)\n",
    "\n",
    "# Showing strides for the 2nd layer (max pooling)\n",
    "print('Strides of the max pooling layer: ', model.layers[1].strides)\n",
    "print()\n",
    "\n",
    "# Showing configurations for entire model\n",
    "# print(model.get_config())\n",
    "\n",
    "# Showing configurations for specific layers\n",
    "print('Full configuration details of the 1st layer:\\n', model.get_config()['layers'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 4: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 5: Overfitting loaded CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Training model\n",
    "h = model.fit(x_train[:100], y_train[:100],\n",
    "                        batch_size=1,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_validation[:500], y_validation[:500]),\n",
    "                        callbacks=[learning_rate],\n",
    "                        verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGB Traffic Signs dataset (255.0 ==> mean ==> std)\n",
    "\n",
    "## Step 6: Showing and plotting accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracies of the model\n",
    "print('Training accuracy={0:.5f}, Validation accuracy={1:.5f}'.\n",
    "                                                             format(max(h.history['accuracy']),\n",
    "                                                                    max(h.history['val_accuracy'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies for every model\n",
    "plt.plot(h.history['accuracy'], '-o')\n",
    "plt.plot(h.history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['training accuracy', 'validation accuracy'], loc='upper left', fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Overfitting model for Traffic Signs Dataset', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "plt.savefig('overfitted_model_of_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'load_model' function:  \n",
    "**print(help(load_model))**  \n",
    "  \n",
    "More details and examples are here:  \n",
    "https://keras.io/api/models/model_saving_apis/#loadmodel-function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(load_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
